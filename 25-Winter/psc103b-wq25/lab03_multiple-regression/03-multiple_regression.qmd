---
title: "Week 3 - Multiple linear regression"
subtitle: "PSC 103B"
author: "Marwin Carmo"
format: 
  revealjs:
    scrollable: true
    incremental: true
    code-tools: true
    code-copy: true
    code-line-numbers: true
    code-link: true
    preview-links: true
    slide-number: true
    self-contained: true
    fig-height: 4
    fig-width: 6
    fig-align: center
    margin-left: "0"
    margin-right: "0"
    width: 1400
    callout-icon: false
    # height: 900
    footer: "PSC 103B - Statistical Analysis of Psychological Data"
    logo: "https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/01-week1-intro/02-code/02-images/ucdavis_logo_blue.png"
editor: source
execute: 
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r packages, echo=FALSE}
library(dplyr)
library(ggplot2)
library(ggpubr)
```

```{r, echo=FALSE}
my_theme <- theme_minimal(base_family = "Roboto Condensed", base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        # Bold, bigger title
        plot.title = element_text(face = "bold", size = rel(1.7)),
        # Plain, slightly bigger subtitle that is grey
        plot.subtitle = element_text(face = "plain", size = rel(1.3), color = "grey70"),
        # Italic, smaller, grey caption that is left-aligned
        plot.caption = element_text(face = "italic", size = rel(0.7), 
                                    color = "grey70", hjust = 0),
        legend.position = c("top"),
        legend.text = element_text(size=12),
        # Bold legend titles
        legend.title = element_text(face = "bold"),
        # Bold, slightly larger facet titles that are left-aligned for the sake of repetition
        strip.text = element_text(face = "bold", size = rel(1.1), hjust = 0),
        # Bold axis titles
        axis.title = element_text(face = "bold"),
        # Add some space above the x-axis title and make it left-aligned
        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),
        axis.text.x = element_text(color = "grey70"),
        # Add some space to the right of the y-axis title and make it top-aligned
        axis.title.y = element_text(margin = margin(r = 10), hjust = 1),
        axis.text.y = element_text(color = "grey70"),
        # Add a light grey background to the facet titles, with no borders
        strip.background = element_rect(fill = "grey90", color = NA))
```


## Today's dataset

```{r, echo=TRUE}
reading <- read.csv("https://shorturl.at/M82EM")
```

`ParentChildAct`: A composite measure of how often parents spend time with reading-related activities to their children, measured in minutes.

`ChildAge`: The child's age in months.

::: {.semi-fade-out}
Five measures of literacy and language skills: `PrintKnowledge`, `ReadingInterest`, `EmergentWriting`, `ExpressLang`, and `ReceptiveLang`.
:::

`OverallLiteracy`: A composite measure averaging the 5 measures of literacy/language.

## Simple linear regression

- Regression examines the relation between an outcome variable and a set of *q* predictor variables.

- In simple linear regression there is only one predictor:

- $y_i = b_0 + b_1x_{1_i} + \epsilon_i$
  - $b_0$ = intercept;
  - $b_1$ = slope of $x_1$;
  - $\epsilon_i$ = residuals.

## Simple linear regression

- In R we use the `lm()` to estimate a regression model

::: fragment
```{r, echo=TRUE}
literacy_age <- lm(OverallLiteracy ~ ChildAge, data = reading)
```
:::

- What function can we use to see the results of the regression analysis?

---

```{r, echo=TRUE}
summary(literacy_age)
```

::: {.notes}
What are the value of the intercept, slope and R-squared? Are the coefficients statistically significant?
:::

## Confidence intervals for the estimates

We can obtain the 95% CI using the `confint()` function:

::: fragment
```{r, echo=TRUE}
confint(object = literacy_age, level = 0.95)
```
:::

- If we were to collect another sample size of 150, measure Child Age and a Literacy score, and estimate $b_1$ over and over again, 95% of the intervals would cover the true value – and 5% would miss it.

## Confidence Limits on Y

- This is analogous to calculating a confidence interval for each of the predicted values $\hat{Y}_i$.

- Given our model, where would we expect to see future observations fall?

## Confidence Limits on Y

Let’s say that we want to calculate the prediction interval for $\widehat{Literacy}_i$ for a child whose parents spend 20 minutes each day reading with them.

```{r, echo=TRUE}
literacy_parent <- lm(OverallLiteracy ~ ParentChildAct, data = reading)
summary(literacy_parent)$coefficients
```

```{r, echo=TRUE}
pred_literacy <- 13.27 + 0.21*20
pred_literacy
```

---

Let's skip the lengthy calculations and use `R`'s `predict()` function:

```{r}
predict(object = literacy_parent, newdata = data.frame( ParentChildAct=20), 
        level = 0.95, interval = "predict")
```

- `object` is our regression model used for making prediction.

- `newdata` is a data frame specifying the values of the predictor variable(s) used in the model.

- `level` is the confidence level for the prediction interval.

- `interval = "predict"` specifies that a prediction interval (for future observations) should be returned.


# Multiple Regression


## Multiple Regression

- When we use multiple regression, we use more than one predictor.

- $y_i = b_0 + b_1x_{1_i} + b_2x_{2_i} + \ldots + b_qx_{q_i} + \epsilon_i$

- Say that we want to use `ParentChildAct` and `ChildAge` simultaneously in the model, we would have:
  - $OverallLiteracy_i = b_0 + b_1 \times ChildAge_i + b_2 \times ParentChildAct_i + \epsilon_i$

## Multiple Regression - running the model

- To run it in R, we just have to add on the predictor to our model

::: fragment

```{r, echo=TRUE}
literacy_multiple <- lm(OverallLiteracy ~ ChildAge + ParentChildAct, data = reading)
```

```{r}
summary(literacy_multiple)$coefficients
```
:::


## Multiple Regression - coefficients

- Now we have more terms: we have the intercept, the effect of age, and the slope associated with the  time parents spend doing reading-related activities.

- Like before, the intercept is the expected value of our outcome when *all* predictors are 0.

- When a child is 0 months old, and a parent spends 0 minutes per day doing reading-related activities with them the expected literacy score is 3.37.

<!-- ## Multiple Regression - coefficients -->

<!-- - The slope is now the expected change in $Y$ for a 1-unit increase in $X$, **holding the other variable constant**. -->

<!-- - It ensures we **isolate** the effect of the specific independent variable we're interested in, without conflating it with the influences of other variables in the model. -->

<!-- :::fragment -->
<!-- ![](img/partial_diagram.png){fig-align="center" width=55%} -->
<!-- ::: -->

## Multiple Regression - coefficients

- The slope for age means that if we keep the amount of time on parent-child activities constant, then increasing child's age by 1 month will increase the literacy score by 0.20.

- Similarly, the slope for parent-child activities means that if we hold the child's age constant,  then increasing the time on reading activities by 1 minute is expected to increase the child's score by 0.21 points.


## Multiple Regression - coefficients

- The slope for `ChildAge` and `ParentChildAct` is the same as in the simple linear regression models. But don't always expect that! That only happens because `ChildAge` and `ParentChildAct` have a correlation of only `r round(cor(reading$ChildAge, reading$ParentChildAct), 3)`.

- There is no overlapping variance between the two, so both have maintained their unique contribution to predict the outcome.

::: {.notes}
Multiple regression lets us examine the unique effect of each variable in our model, given all the other variables; therefore, if the slope is still significant, it means that there is something unique about our predictor that is predicting the outcome.
:::

## Multiple Regression - $R^2$

- What is our $R^2$ now?

::: fragment
```{r, echo=TRUE}
summary(literacy_multiple)$r.squared
```
:::

- Compare the previous model and take note of what happened. Can you guess why?

::: fragment
```{r, echo=TRUE}
summary(literacy_age)$r.squared

summary(literacy_parent)$r.squared
```
:::

## Centering predictors

- You might have noticed that the intercept doesn't always make sense.

- Are we ever expected to have a child who is 0 months old? No! That child isn't even born yet!

<!-- ::: fragment -->
<!-- ```{r, message=FALSE} -->
<!-- #| fig-align: "center" -->
<!-- #| out-width: 40% -->

<!-- ggplot(data = reading, aes(y = OverallLiteracy, x = ChildAge)) +  -->
<!--   geom_point(alpha = .5) + -->
<!--   theme_minimal() + -->
<!--   ylab('Literacy') + -->
<!--   xlab('Child age (month)') + -->
<!--   geom_smooth(method = 'lm', se = F, fullrange = TRUE) + -->
<!--   geom_vline(xintercept = 0, linetype = "dashed") +  -->
<!--   geom_segment(aes(xend = ChildAge, yend = literacy_age$fitted.values), color = "red",linetype = "dashed", alpha=.3 ) + -->
<!--   coord_cartesian(xlim = c(0, 66)) -->
<!-- ``` -->
<!-- ::: -->

## Centering predictors

```{r}
reading$ChildAge_c <- scale(reading$ChildAge, scale = FALSE)
```

Centering around the mean shifts all the values of your variable so that the overall distribution stays the same but the mean of that distribution is now 0.

::: fragment
```{r, echo=TRUE}
reading$ChildAge_c <- reading$ChildAge - mean(reading$ChildAge)
# OR with the scale() function
reading$ChildAge_c <- as.vector(scale(reading$ChildAge, scale = FALSE))
```
:::

- Notice what happens

::: fragment
```{r, echo=TRUE}
head(reading$ChildAge)
head(reading$ChildAge_c)
```
:::

## Centering predictors

```{r}
#| fig-align: "center"
#| echo: false

g1 <- ggplot(data = reading, aes(y = OverallLiteracy, x = ChildAge)) + 
  geom_point(alpha = .5) +
  theme_minimal() +
  ylab('Literacy') +
  xlab('Child age (month)') +
  geom_smooth(method = 'lm', se = F) +
  geom_vline(xintercept = 0, linetype = "dashed") + 
  coord_cartesian(xlim = c(-23, 66))

# Mean-centered ChildAge
g2 <- ggplot(data = reading, aes(y = OverallLiteracy, x = ChildAge_c)) + 
  geom_point(alpha = .5) +
  theme_minimal() +
  ylab('Literacy') +
  xlab('Centered Child age (month)') +
  geom_smooth(method = 'lm', se = F) +
  geom_vline(xintercept = 0, linetype = "dashed") + 
  coord_cartesian(xlim = c(-23, 66))
  
ggarrange(g1, g2, nrow = 2)
```

```{r, eval=FALSE}
#| fig-align: "center"
#| out-width: 70%
#| echo: false

p_age1 <- ggplot(reading, aes(x = ChildAge)) + 
  geom_histogram(#bins = 30, 
                 color = "skyblue", fill = "lightblue") +
  labs(title = "Histogram of Child Age", 
       x = "Child Age", 
       y = NULL)

p_age2 <- ggplot(reading, aes(x = ChildAge_c)) + 
  geom_histogram(#bins = 30, 
                 color = "skyblue", fill = "lightblue") +
  labs(#title = "Histogram of Child Age", 
       x = "Child Age centered", 
       y = NULL)

ggarrange(p_age1, p_age2, ncol = 2)
```

## Centering predictors

Let's also center our parent-child activities variable:

```{r, echo=TRUE}
reading$ParentChildAct_c <- reading$ParentChildAct - mean(reading$ParentChildAct)
```

Let's re-run our regression to see what changes

```{r, echo=TRUE}
literacy_multiple_centered = lm(OverallLiteracy ~ ChildAge_c + ParentChildAct_c, data = reading)
```

::: fragment
```{r}
summary(literacy_multiple_centered)$coefficients
```
:::

## Centering predictors

$$
Y_i = b_0 + b_1 (X_{1_i} - \bar{X_1}) + b_2 (X_{2_i} - \bar{X_2})+ \epsilon_i
$$

- Notice that our slopes have not changed.

- The intercept still represents the expected academic performance when all our predictors are 0.

- But now our predictors are centered, so the predictors are only 0 when the variables are equal to the average value.


## Centering predictors

This makes the intercept more useful: it is the expected literacy when we have a child who is the **average age** and whose parents spend the **average amount of time doing reading related activities** with them.

## Interactions

- Interactions are how we can determine whether there is a moderation effect.

- Moderation between 2 predictor variables is like  saying "depends on" - the effect of one predictor variable "depends on" what value the other predictor takes.

## Interactions

- Perhaps the effect of parent-child activities depends on the child's age.

- We might want to test if there is an interaction between child's age and parent-child activities.

<!-- - To test an interaction in R, you just need to multiply your 2 predictors together instead of adding: -->

:::fragment
```{r, echo=TRUE}
interaction_model <- lm(OverallLiteracy ~ ChildAge_c * ParentChildAct_c, data = reading)
```

```{r}
summary(interaction_model)$coefficients
```
:::

## Interactions

- Is our interaction term significant? 

- No. So that means the effect of parent-child activities **does not depend** on how old the child is -- it's equally important at all ages.

- *If* it was a significant effect, we could conclude that the effect of parent-child activities on child literacy depends on how old a child is.