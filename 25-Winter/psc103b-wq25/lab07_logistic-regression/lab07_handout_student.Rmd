---
title: "Lab 05 - Post-hoc Tests & Factorial ANOVA"
author: "Marwin Carmo"
date: "2025-02-13"
output: pdf_document
subtitle: "PSC-103B"
affiliation: The University of California, Davis
editor_options:
  chunk_output_type: inline
  markdown: 
    wrap: 80
---

Today, we will be learning about logistic regression which is when you want to fit a regression model to some binary data, which only takes on values of 0 or 1. Remember that oftentimes, 1 corresponds to a "success" of some
type, whether that is getting into graduate school, or
receiving a promotion, or flipping a heads on a coin (it's
all about how you want to define success).

Since our outcome is binary, we can't use our traditional
linear regression model to predict a success or failure.
This is because we're trying to predict a probability
of success, and probability is bounded between 0 and 1.
Linear regression assumes your outcome is continuous, or
unbounded, so we need a model that respects the fact
that probability is bounded.

Enter logistic regression! But as we saw in class, logistic
regression doesn't predict probability directly
because the regression equation would be complicated to estimate.
Instead, logistic regression predicts the log-odds of success
because there is a linear relationship between the log-odds
and your predictors, so we can use OLS regression
to estimate the model (which is simpler!)

We are going to demonstrate logistic regression with
this dataset on red wine quality, which can be found at
<https://archive.ics.uci.edu/ml/datasets/wine+quality>.

This dataset contains information on characteristics of
the wine, such as the acidity, sugar, pH, alcohol content, etc.
as well as a rating of the wine as "good" or "bad", which 
was created by dichotomizing a quality variable (so that 
anything with a quality score greater than 5 out of 9
was considered good).

Let's load it in.

```{r}
wine <- read.csv("https://shorturl.at/Vwcr6")

head(wine)
```

Notice that right now, the quality variable has two options
bad or good; to make this easier for us, we are going 
to create a second column that assigns this a numerical
value (1 if the quality is good, 0 if it's bad).
That way, we can be sure our logistic regression is predicting
the log-odds of a wine being good.

```{r}
wine$quality_binary <- ifelse(wine$quality == "good", 1, 0)
```

Did this work? 

```{r}
head(wine)
```

## Simple Logisitic Regression

To start, let's fit a logistic regression model with only
one predictor - what if we thought that alcohol content
was a good indicator of whether or not a red wine
was considered good or bad?

You might remember that we fit linear regression models
using the `lm()` function. However, since we need to fit
a logistic regression function, we need to use a new
function called `glm()`.
This function, for the most part, is similar to the `lm()`
function: you specify a formula in the form `outcome var ~ independent var(s)`
and then you provide some data.
The only extra thing you have to do now is specify a family,
using `family = "binomial"` (this tells `R` that the outcome
variable is binary, and to use logistic regression).

Fit the model and save it to `simple_logreg`:

```{r Exercise-1}
# your code goes here

```

Just like in linear regression, we can access information about the model using the `summary()` function:

```{r}
summary(simple_logreg)
```

Let's write out this model and interpret it

$$
\log(Odds_{\text{Quality}}) = -10.76 + 1.06 \times \text{Alcohol}
$$

The intercept of -10.76 means that a red wine that has 0 
alcohol content has an expected log-odds score of -10.76.
The slope means that for every 1-unit increase in alcohol
content, the log-odds of a wine being rated good increase
by 1.06 points.

Hopefully this interpretation seems pretty similar to how 
we interpreted things in linear regression. Only now
our outcome variable is the log-odds of something being
successful (here, the log-odds of a wine being rated good)
and that's a little bit difficult for people to interpret
because it's hard for us to have an intuitive sense of 
these things.

So often, people will transform the coefficients
to be interpreted in terms of odds ratios.
And you can do that by simply exponentiating! 

If we were to re-write this model in terms of the odds
we would have:

$$
Odds_{\text{Good}} = \exp(-10.76 + 1.06 \times \text{Alcohol})
$$

or (using the rules of exponents),

$$
Odds_{\text{Good}} = \exp(-10.76) \times \exp(1.06 \times \text{Alcohol})
$$

So what do these values mean now?

Well, our intercept is still the expected value when
alcohol is 0.
So when a wine has no alcohol content, the expected
odds of being rated good are exp(-10.76), or .00002 (these
odds are super tiny -- people don't like wine with no alcohol!).
Now, the slope is exp(1.06), and is the multiplicative change
in the odds for a 1-unit change in Alcohol.
So when alcohol content increases by 1, the odds increase
by a factor of (are multiplied by) exp(1.06) = 2.89.
This is almost a 300% increase in the odds!
Because 1.06 is positive, we know that as the alcohol
content increases, the probability of a wine being considered
good also increase.

Just like in linear regression, we can mean-center our
predictor variables to improve the interpretation
of our intercept -- because a wine with an alcohol content
of 0 is pretty unlikely (unless its a special, NA wine).

Let's create a centered version of alcohol:

```{r}
wine$alcohol_c <- wine$alcohol - mean(wine$alcohol, na.rm = TRUE)
```

And use that as a predictor,

```{r}
logreg_centered <- glm(quality_binary ~ alcohol_c,
                      data = wine,
                      family = "binomial")

summary(logreg_centered)
```

Notice that the value of the slope hasn't changed, 
but now our intercept is 0.24 -- what does this mean?
It means the expected log-odds of a wine with an average alcohol
content being rated good is 0.24
or that the odds of it being rated good are exp(.24) = 1.27
so it's more likely to be rated good than it is to be rated bad.

One other thing you can look at with the output of a logistic 
regression is the predicted probability - for a particular
alcohol content, what is the probability that the wine will
be rated "good"? 
Let's take the example of a Cabernet Sauvignon, which 
generally has an alcohol content of 14%.

What are the predicted log-odds of that wine being rated "good"?

```{r Exercise-2a}
# your code goes here
```

the expected log-odds are 4.08, which again, are hard for us to interpret so let's transform those into `odds`.

```{r Exercise-2b}
# your code goes here
```

the expected odds of a cab being rated good is **huge**, 59.15.
A cab is 59 times as likely to be rated good as it is to be
rated bad.

Finally, we can transform these odds into a probability using, $P = \frac{odds}{1 + odds}$

```{r}
prob <- odds / (1 + odds)
prob
```
A cab has almost 100% (98.34%) chance of being rated good according to our model.

Some rules that might be helpful to know
is that a probability of 0.5 (equal chance) corresponds
to an odds ratio of 1, which corresponds to a log-odds of 0.
Therefore, if something has a log-odds greater than 0,
then that **increases** your chance of a success (probability
greater than 0.5), and a log-odds less than 0 **decreases**
your chance of success (probability less than 0.5).

Now you try -- fit a model predicting quality
from the amount of chlorides, which can affect how salty
a wine tastes.
And interpret the coefficients in terms of log-odds and odds.

```{r}
log_fit <- glm(quality_binary ~ chlorides,
              data = wine,
              family = "binomial")
summary(log_fit)
```

## Multiple Logistic Regression

Just like linear regression had a "simple" and "multiple"
version based on the number of predictors, we can 
add multiple predictors to our logistic regression model.
Let's include alcohol content in our logistic regression
model, like before, but now add sulphates.

```{r}
multiple_logreg <- glm(quality_binary ~ alcohol + sulphates,
                      data = wine,
                      family = "binomial")
summary(multiple_logreg)
```

What is our regression model? And what do the values mean?

$$
\log(Odds_{\text{Good}}) = -12.32 + 1.04 \times \text{Alcohol} + 2.67 \times \text{Sulphates}
$$

- Intercept: A wine with no alcohol content and no sulphates has a log-odds of -12.32.
- Slope of Alcohol: For each 1-unit increase in alcohol content, holding amount of sulphates constant, the log-odds of a wine
being rated good increase by 1.03.
- Slope of Sulphates: For each 1-unit increase in sulphates,
holding alcohol content constant, the log-odds increase by 2.67.

But let's interpret these in terms of the odds
 
- Intercept: The expected odds of a wine with no alcohol content and no sulphates is exp(-12.32) = .000004 (again, almost 0!)
- Slope of Alcohol: Holding the amount of sulphates constant,
increasing the alcohol content by 1 increases the odds of a
wine being rated as good by a factor of exp(1.04) = 2.83. The odds are multiplied by 2.83 for each 1-unit increase
in alcohol content, holding the amount of sulphates constant **or**
The odds increase by 183% for each 1-unit increase in alcohol
content, holding the amount of sulphates constant.
- Slope of Sulphates: Holding the amount of alcohol constant,
increasing the sulphates by 1 unit increases the odds of a
wine being rated as good by a factor of exp(2.67) = 14.44. The odds are multiplied by 14.44 for each 1-unit increase
in sulphate content, holding the amount of alcohol constant **or**
The odds increase by 1343% for each 1-unit increase in sulphate
content, holding the amount of alcohol constant.

Of course, you can also add interactions to your 
logistic regression model.

```{r}
interact_logreg <- glm(quality_binary ~ alcohol + sulphates + I(alcohol*sulphates),
                      data = wine,
                      family = "binomial")
summary(interact_logreg)
```

We're not going to interpret the values of this model,
other than to say that the significant interaction
means that the effect of alcohol on the log-odds (or odds,
or probability) of a wine being rated good depends on
the amount of sulphates (or vice versa).
What is that dependence? You'd have to plot the relation
between alcohol and the log-odds for a few different values 
of sulphates to find out.

## General Linear Model

One final thing you all learned about was the 
general linear model, and how a lot of the tests
you guys have learned about in class -- t-tests and ANOVA -- 
can be rewritten as linear regressions with the help
of dummy coding.

Let's revisit our ANOVA example from a few weeks ago
where we wanted to examine bill length differences
between different species of penguins.
And we had to do an F-test to determine whether any of the means
were different, and then a post-hoc test to see which means
were different.

```{r}
# Load the palmerpenguins package
library(palmerpenguins)
```

But we can fit this as a regression model with dummy codes
and this might help us make more specific comparisons
right away, particularly if there was one single group
like a control group, that we were interested in comparing
our other groups to.

A dummy code is a variable that assigns a value of 1 
if a person is in one specific group, and 0 otherwise.
You can manually create these dummy codes, or if you 
fit a regression model with a "factor" variable as a
predictor, R will automatically create dummy codes.

```{r}
dummycode <- lm(bill_length_mm ~ species, data = penguins)
summary(dummycode)
```

So what does this output mean?
Well, `R` automatically assigned one group to be our reference group
and that's generally done by choosing the group that comes
first alphabetically (in this case, Adelie).
This reference group has a score of 0 on the dummy code
variables.
And now the slopes are the effects of the dummy codes
for the other 2 species.

So our intercept now represents the expected bill length
for a penguin that has a 0 on both dummy codes -- in other
words, a penguin in the reference group, Adelie! 
The value of 38.79 is the average bill length for Adelie
penguins.
What do the slopes represent? The difference between the mean
of the reference group, and the mean of the group that the 
dummy code is for.

In other words, the slope of 10.04 for `speciesChinstrap`
means that Chinstrap penguins have an average bill length
that is 10.04 mm longer than the average bill length of Adelie
penguins.
And the value of 8.71 for `speciesGentoo` means that Gentoo 
penguins have an average bill length that is 8.71 mm
longer than the average bill length of Adelie penguins.

We can see if this matches:

```{r}
tapply(penguins$bill_length_mm, penguins$species, mean,
       na.rm = TRUE)
```

Both these differences are significant! So unlike before
where we had to do a post-hoc test to see which groups
were different, we can automatically see which pairs are
significantly different.

Of course, the only downside to this approach is that not all
comparisons are represented - we can't say anything about
the difference between Chinstrap and Gentoo penguins.
That's why this approach is often performed for comparisons
among some type of experimental group and a clear control group!