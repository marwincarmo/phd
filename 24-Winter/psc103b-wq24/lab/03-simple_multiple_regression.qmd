---
title: "Week 3 - Simple and multiple linear regression"
subtitle: "PSC 103B"
author: "Marwin Carmo"
format: 
  revealjs:
    scrollable: true
    incremental: true
    code-tools: true
    code-copy: true
    code-line-numbers: true
    code-link: true
    preview-links: true
    slide-number: true
    self-contained: true
    fig-height: 4
    fig-width: 6
    fig-align: center
    margin-left: "0"
    margin-right: "0"
    width: 1400
    # height: 900
    footer: "PSC 103B - Statistical Analysis of Psychological Data"
    logo: "https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/01-week1-intro/02-code/02-images/ucdavis_logo_blue.png"
editor: source
editor_options: 
  chunk_output_type: console
filters:
  - ../homework/color.lua
---

```{r packages, echo=FALSE}
library(dplyr)
library(ggplot2)
library(ggpubr)
```

## Today's dataset

```{r, echo=TRUE}
reading <- read.csv("data/Lab3Data.csv", header = TRUE)
```

`ParentChildAct`: A composite measure of how often parents spend time with reading-related activities to their children, measured in minutes.

`ChildAge`: The child's age in months.

Five measures of literacy and language skills: `PrintKnowledge`, `ReadingInterest`, `EmergentWriting`, `ExpressLang`, and `ReceptiveLang`.

`OverallLiteracy`: A composite measure averaging the 5 measures of literacy/language.

## Simple linear regression

```{r, echo=FALSE}

literacy_age <- lm(OverallLiteracy ~ ChildAge, data = reading)
```

::: columns
::: {.column width="50%"}
- Regression examine the relation between an outcome variable and a set of *q* predictor variables.

- In simple linear regression there is only one predictor

- $y_i = b_0 + b_1x_1 + \epsilon_i$
:::

::: {.column width="50%"}
::: fragment
```{r, message=FALSE}
ggplot(data = reading, aes(y = OverallLiteracy, x = ChildAge)) + 
  geom_point(alpha = .5) +
  theme_classic() +
  ylab('Literacy') +
  xlab('Child age (month)') +
  geom_smooth(method = 'lm', se = F, fullrange = TRUE) +
  geom_vline(xintercept = 0, linetype = "dashed") + 
  geom_segment(aes(xend = ChildAge, yend = literacy_age$fitted.values), color = "red",linetype = "dashed", alpha=.3 ) +
  coord_cartesian(xlim = c(0, 66))
```
:::
:::
:::

## Simple linear regression

- In R we use the `lm()` to estimate a regression model

- `lm(dependent_variable ~ independent_variable, data = data_name)`

- Fit a linear model with the `lm()` function predicting `OverallLiteracy` from `ChildAge` and save to an object called `literacy_age`. 

::: fragment
```{r, echo=TRUE}
literacy_age <- lm(OverallLiteracy ~ ChildAge, data = reading)
```
:::

- What function can we use to see the results of the regression analysis?

## The `lm()` output

::: fragment
```{r, echo=TRUE}
summary(literacy_age)
```
:::

---
```{r}
literacy_parent <-  lm(OverallLiteracy ~ ParentChildAct, data = reading)
```

Now fit a linear model predicting `OverallLiteracy` from `ParentChildAct`.


- What is the estimate of the y-intercept for the model, rounded to two decimal places? What does this number mean?

::: {.fragment .fade-in}
[13.27. This means that when the parent and child activity is 0 we expect the child's literacy score to be 13.27.]{color=#b22222}
:::

- If the GLM for this model is $OverallLiteracy_i = b_0 + b_1 \times ParentChildAct_i + \epsilon_i$, then $b_1$ is? What does it mean?

::: {.fragment .fade-in}
$b_1$ = 0.21. For each minute increase in parent and child activity, the child literacy is expected to increase by 0.21 units.
:::

- Is the overall model significant? If so, what proportion of the variance does the model explain?

::: {.fragment .fade-in}
The model is statistically significant and and explains 11% of the total variance ($R^2$ = 0.11, *F*(1, 148) = 18.00, *p* < .001).
:::

## Confidence intervals for the estimates

We can obtain the 95% CI using the `confint()` function:

::: fragment
```{r, echo=TRUE}
confint(object = literacy_parent, level = 0.95)
```
:::

- If we were to collect another sample size of 150, measure Parent-child activity and a Literacy score, and estimate $b_1$ over and over again, 95% of the intervals would cover the true value – and 5% would miss it.

## Confidence Limits on Y

- This is analogous to calculating a confidence interval for each of the predicted values $\hat{Y}_i$.

- Given our model, where would we expect to see future observations fall?

```{r, fig.align="center", fig_width = "800px"}


ggplot(data = reading, aes(y = OverallLiteracy, x = ParentChildAct)) + 
  geom_point() +
  theme_minimal() +
  xlab('Parent-Child Activities') +
  ylab('Overall Literacy') +
  geom_smooth(method = 'lm', se = TRUE)
```


## Confidence Limits on Y

Let’s say that we want to calculate the prediction interval for $\widehat{Literacy}_i$ for a child whose parents spend 20 minutes each day reading with them.

::: fragment
$$
\hat{Y}_i = 13.27 + 0.21X_i
$$
:::

::: fragment
### Step 1: calculate $\hat{Y}*$

```{r, echo=TRUE}
pred_literacy <- 13.24 + 0.21*20
pred_literacy
```

:::

## Confidence Limits on Y

::: fragment

### Step 2: calculate $s_{\hat{Y}*}$

- $s_{\hat{Y}*} = s_\epsilon\sqrt{\frac{1}{N}+\frac{(X*-\bar{X})^2}{SS_X}}$

- $s_\epsilon$:
:::

::: fragment
```{r, echo=TRUE}
sqrt( (1/(nrow(reading) - 2)) * sum((reading$OverallLiteracy - predict(literacy_parent))^2) )
# OR
s_epsilon <- summary(literacy_parent)$sigma
```

:::

- $SS_X$:

::: fragment
```{r, echo=TRUE}
ss_x <- sum((reading$ParentChildAct - mean(reading$ParentChildAct))^2)
```
:::

## Confidence Limits on Y

```{r, echo=TRUE}
s_y <- s_epsilon * sqrt( (1/nrow(reading)) + (( (20 - mean(reading$OverallLiteracy))^2 ) / ss_x))
```

```{r, echo=TRUE}
t_crit <- qt(.025, nrow(reading) - 2, lower.tail = FALSE)
```


```{r, echo=TRUE}
pred_literacy + t_crit*s_y
```


## Write-up the results

We ran a simple linear regression to determine whether Parent and Child activity predicts child reading literacy.
The effect of Parent and Child activity is statistically significant and positive (*b* = 0.21, 95% CI
[0.11, 0.31], *t*(148) = 4.24, *p* < .001). Parent and Child activity explained 11% of the total variability in child reading literacy.


## Multiple Regression

- When we use multiple regression, we use more than one predictor.

- $y_i = b_0 + b_1x_{1_i} + b_2x_{2_i} + \ldots + b_qx_{q_i} + \epsilon_i$

- Say that we want to use `ParentChildAct` and `ChildAge` simultaneously in the model, we would have
  - $OverallLiteracy_i = b_0 + b_1 \times ChildAge_i + b_2 \times ParentChildAct_i + \epsilon_i$

## Multiple Regression - running the model

- To run it in R, we just have to add on the predictor to our model

```{r, echo=TRUE}
literacy_multiple <- lm(OverallLiteracy ~ ChildAge + ParentChildAct, data = reading)

```


```{r}
summary(literacy_multiple)
# broom::tidy(literacy_multiple) |> 
#   kableExtra::kbl()
```

## Multiple Regression - coefficients

- Now we have more terms: we have the intercept, the effect of age, and the slope associated with the  time parents spend doing reading-related activities.

- Like before, the intercept is the expected value of our outcome when *all* predictors are 0.

- when a child is 0 months old, and a parent spends 0 minutes per day doing reading-related activities with them the expected literacy score is 3.37.

## Multiple Regression - coefficients

- What has changed is the interpretation of the slope.

- The slope is now the expected change in Y for a 1-unit increase in X, **holding the other variable constant**

- It ensures we isolate the effect of the specific independent variable we're interested in, without conflating it with the influences of other variables in the model.

## Multiple Regression - coefficients

- The slope for age means that if we keep the amount of time on parent-child activities constant, then increasing child's age by 1 month will increase the literacy score by 0.20.

- Similarly, the slope for parent-child activities means that if we hold the child's age constant,  then increasing the time on reading activities by 1 minute is expected to increase the child's score by 0.21 points.

- Are these slopes significant?

## Multiple Regression - coefficients

- Multiple regression lets us examine the unique effect of each variable in our model, given all the other variables; therefore, if the slope is still significant, it means that there is something unique about our predictor that is predicting the outcome.

- The slope for `ChildAge` and `ParentChildAct` is the same as in the simple linear regression models. But don't always expect that! That only happens because `ChildAge` and `ParentChildAct` have a correlation of only `r round(cor(reading$ChildAge, reading$ParentChildAct), 3)`.

- There is no overlapping variance between the two, so both have maintain their unique contribution to predict the outcome.

## Multiple Regression - R-squared

- What is our $R^2$ now?

::: fragment
```{r, echo=TRUE}
summary(literacy_multiple)$r.squared
```
:::

- Compare the previous model and take note of what happened. Can you guess why?

::: fragment
```{r, echo=TRUE}
summary(literacy_age)$r.squared

summary(literacy_parent)$r.squared
```
:::

## Centering predictors

- You might have noticed is that the intercept doesn't always make sense.

- Are we ever expected to have a child who is 0 months old? No! That child isn't even born yet!

::: fragment
```{r, message=FALSE}
ggplot(data = reading, aes(y = OverallLiteracy, x = ChildAge)) + 
  geom_point(alpha = .5) +
  theme_minimal() +
  ylab('Literacy') +
  xlab('Child age (month)') +
  geom_smooth(method = 'lm', se = F, fullrange = TRUE) +
  geom_vline(xintercept = 0, linetype = "dashed") + 
  geom_segment(aes(xend = ChildAge, yend = literacy_age$fitted.values), color = "red",linetype = "dashed", alpha=.3 ) +
  coord_cartesian(xlim = c(0, 66))
```
:::

## Centering predictors

```{r}
reading$ChildAge_c <- scale(reading$ChildAge, scale = FALSE)
```

Centering around the mean shifts all the values of your variable so that the overall distribution stays the same but the mean of that distribution is now 0.

```{r}
#| fig-align: "center"

g1 <- ggplot(data = reading, aes(y = OverallLiteracy, x = ChildAge)) + 
  geom_point(alpha = .5) +
  theme_minimal() +
  ylab('Literacy') +
  xlab('Child age (month)') +
  geom_smooth(method = 'lm', se = F) +
  geom_vline(xintercept = 0, linetype = "dashed") + 
  coord_cartesian(xlim = c(-23, 66))

# Mean-centered ChildAge
g2 <- ggplot(data = reading, aes(y = OverallLiteracy, x = ChildAge_c)) + 
  geom_point(alpha = .5) +
  theme_minimal() +
  ylab('Literacy') +
  xlab('Centered Child age (month)') +
  geom_smooth(method = 'lm', se = F) +
  geom_vline(xintercept = 0, linetype = "dashed") + 
  coord_cartesian(xlim = c(-23, 66))
  
ggarrange(g1, g2, nrow = 2)
```

## Centering predictors

We can center a variable in two ways:

::: fragment
```{r, echo=TRUE}
reading$ChildAge_c <- reading$ChildAge - mean(reading$ChildAge)
# OR with the scale() function
reading$ChildAge_c <- scale(reading$ChildAge, scale = FALSE)
```
:::

- Notice what happens

::: fragment
```{r, echo=TRUE}
head(reading$ChildAge)
head(reading$ChildAge_c)
```
:::

## Centering predictors

```{r}
#| fig-align: "center"
#| out-width: 70%
par(mfrow = c(1, 2))
hist(reading$ChildAge)
hist(reading$ChildAge_c)
```

## Centering predictors

Let's also center our parent-child activities variable:

```{r, echo=TRUE}
reading$ParentChildAct_c <- reading$ParentChildAct - mean(reading$ParentChildAct)
```

Let's re-run our regression to see what changes

```{r, echo=TRUE}
literacy_multiple_centered = lm(OverallLiteracy ~ ChildAge_c + ParentChildAct_c,
                           data = reading)
```

::: fragment
```{r}
broom::tidy(literacy_multiple_centered) |> kableExtra::kbl()
```
:::

## Centering predictors

$$
Y_i = b_0 + b_1 (X_{1_i} - \bar{X_1}) + b_2 (X_{2_i} - \bar{X_2})+ \epsilon_i
$$

- Notice that our slopes have not changed.

- The intercept still represents the expected academic performance when all our predictors are 0.

- But now our predictors are centered, so the predictors are only 0 when the variables are equal to the average value.


## Centering predictors

This makes the intercept more useful: it is the expected literacy when we have a child who is the average age and whose parents spend the average amount of time doing reading related activities with them.


