---
title: "A holistic view of academic performance: Beyond Averages with MELSM and Spike-and-Slab"
subtitle: "Quantitative Research Seminar - 05/02/2024"
author: "Marwin Carmo"
format: 
  revealjs:
    scrollable: true
    incremental: true
    code-tools: true
    code-copy: true
    code-line-numbers: true
    code-link: true
    preview-links: true
    slide-number: true
    self-contained: true
    fig-height: 4
    fig-width: 6
    fig-align: center
    margin-left: "0"
    margin-right: "0"
    width: 1400
    # height: 900
    footer: "Beyond Averages with MELSM and Spike-and-Slab"
    logo: "https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/01-week1-intro/02-code/02-images/ucdavis_logo_blue.png"
editor: source
editor_options: 
  chunk_output_type: console
---

## Motivation

- Average performance does not provide information on the *consistency* of academic achievement over time or within a cluster.

- Consider two schools with a final average grade of 75% at the year's end:
  - One had scores ranging from 50% to 100%.
  - The other maintained a steady 75%.
  
- Their level of mastery of different topics are almost certainly different.

## Motivation

- Inconsistent performance may reflect unaccounted factors influencing learning.

-  Understanding the nature of inconsistency may shed light on systemic issues within the school environment that not only affect academic outcomes, but also contribute to students’ social and emotional development (Spörlein & Schlueter, 2018).

```{r eval=FALSE}
set.seed(1235)
library(ggplot2)

df <- data.frame(
  id = 1:40,
  grades1 = pmax(pmin(round(rnorm(40, mean = .60, sd = .10), 2), 1), 0),
  grades2 = pmax(pmin(round(rnorm(40, mean = .75, sd = .35), 2), 1), 0)
)

#df$g1_color = ifelse(df$grades1 < 0.5, "Fail", "Pass")

ggplot(df, aes(x = id, y = grades1)) +
  geom_line( color="grey") +
  geom_point(shape=21, color="black", fill="#69b3a2", size=4) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color="red") +
  geom_hline(yintercept = mean(df$grades1),  color="darkblue") +
  theme_minimal()
```


## Our goal

- Propose a methodology to identify clustering units (students, classrooms, etc.) that exhibit either unusually large or unusually small within-cluster variance.

- We present an adaptation of the Mixed effects location scale model (MELSM) by means of shrinking random effects to their fixed effect using the Spike and Slab regularization

## MELSM

- Educational data is classically analyzed by means of multilevel or mixed effects models (MLM):
  - Schools are level-2 units and students' test scores are the level-1 units.

- MLM assumes constant residual variance (scale)

- MELSM include a sub-model to addresses potential differences in the residual variance.

- The unexplained residual variance might provide insights over and above the expected academic achievement that is captured in the conditional means.

## MELSM

- Does not model the residuals themselves but, instead, the error **variance** ($\sigma^2$) from which the residuals are realized.

- It *simultaneously* estimates the means and the residual variance.

- The model specifies a classic multilevel model for the observed values, $y_{ij}$ for school $j$ and student $i$, and a multilevel model for the within-cluster residual variances $\sigma^2_{\epsilon_{ij}}$.

## Advantages of MELSM

- Account for possible correlations among location and scale effects.

- For example, the variability of student performance within a school can be modeled as a function of parental socioeconomic status.

- Overcomes the limitations of multi-stage approaches and outperform the classic mixed effects model in in- and out of sample performance.

---

The starting point is the standard linear mixed effects model for $i = 1,2,...,n_j$ students, and $j = 1, 2, \ldots, S$ schools, specified as


\begin{equation}
  \label{eq:mixedmodel-lvl1}
  \textrm{Level 1:} \quad y_{ij} = \beta_{0j} + \beta_{1j} X_{ij} + e_{ij}
\end{equation}

\begin{aligned}
  \label{eq:mixedmodel-lvl2}
  \textrm{Level 2:}\quad \beta_{0j} &= \gamma_{00} + \gamma_{01}W_{1j} + u_{0j} \\
                       \beta_{1j} &= \gamma_{10} + \gamma_{11}W_{2j} + u_{1j}
\end{aligned}

\begin{equation}
  \label{eq:bcov}
  \begin{pmatrix} u_0 \\ u_1 \end{pmatrix} \sim \mathcal{N}
    \begin{Bmatrix} 
        \begin{pmatrix} 0 \\ 0  \end{pmatrix},
        \begin{pmatrix} \tau^2_{u_{0}} \\ \tau_{u_{0}u_{1}} & \tau^2_{u_{1}}
        \end{pmatrix}
    \end{Bmatrix}
\end{equation}

\begin{equation}
  e_{ij} \sim \mathcal{N} (0, \sigma_{\epsilon}^2)
\end{equation}

---

With MELSM we also allow the residual variance to differ among $i$-students within $j$-schools to obtain $\sigma^2_{\epsilon_{ij}}$

\begin{equation}
  \label{eq:var-lvl1}
  \textrm{Level 1:} \quad \sigma^2_{\epsilon_{ij}} = \exp(\alpha_{0j} + \alpha_{1j}M_{ij})
\end{equation}

\begin{aligned}
  \label{eq:var-lvl2}
  \textrm{Level 2:}\quad \alpha_{0j} &= \eta_{00} + \eta_{01}P_{1j} + t_{0j} \\
                       \alpha_{1j} &= \eta_{10} + \eta_{11}P_{2j} + t_{1j}
\end{aligned}

---

The random effects from the *scale* and *location* of the model come from a multivariate Gaussian Normal distribution with zero means:

\begin{equation}
  \label{eq:random-cov}
  \textbf{v}=
  \begin{bmatrix} u_0 \\ u_1 \\ t_0 \\ t_1 \end{bmatrix} \sim \mathcal{N}
    \begin{Bmatrix} 
        \boldsymbol{0}=
        \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix},
        \boldsymbol{\Sigma}=
        \begin{bmatrix} \tau^2_{u_0} \\ 
                        \tau_{u_0u_1} & \tau^2_{u_1} \\
                        \tau_{u_0t_0} & \tau_{u_1t_0} & \tau^2_{t_0} \\
                        \tau_{u_0t_1} & \tau_{u_1t_1} & \tau_{t_0t_1} & \tau^2_{t_1}
        \end{bmatrix}
    \end{Bmatrix}
\end{equation}

---

\begin{equation}
\boldsymbol{\Sigma}=
        \begin{bmatrix} \tau^2_{u_0} \\ 
                        \tau_{u_0u_1} & \tau^2_{u_1} \\
                        \tau_{u_0t_0} & \tau_{u_1t_0} & \tau^2_{t_0} \\
                        \tau_{u_0t_1} & \tau_{u_1t_1} & \tau_{t_0t_1} & \tau^2_{t_1}
        \end{bmatrix}
\end{equation}

- $\boldsymbol{\Sigma}$ can be decomposed into $\boldsymbol\Sigma = \boldsymbol{\tau}\boldsymbol{\Omega\tau}'$ to specify independent priors for each element of $\boldsymbol{\tau}$ and $\boldsymbol{\Omega}$.

- $\boldsymbol{\tau}$ is a diagonal matrix of the random-effects standard deviations.

- $\boldsymbol{\Omega}$ is the correlation matrix among all random effects.

## Cholesky transformation

 Next, we can factorize $\boldsymbol\Omega$ via the Cholesky $\textbf{L}$ of $\boldsymbol\Omega = \textbf{L}'\textbf{L}$.

\begin{equation}
\label{eq:cholesky_approach}
\textbf{L} = 
    \begin{pmatrix}
        1 & 0 & 0 & 0\\
        \rho_{u_0u_1} & \sqrt{1 - l^2_{21}} & 0 & 0 \\
        \rho_{u_0t_0} & \frac{\rho_{u_1t_0} - l_{21} l_{31}}{l_{22}} & \sqrt{1 - l^2_{31} - l^2_{32}} & 0\\
        \rho_{u_0t_1} & \frac{\rho_{u_1t_1} - l_{21} l_{41}}{l_{22}} & \frac{\rho_{t_0t_1} - l_{31} l_{41} - l_{32} l_{42}}{l_{33}} & \sqrt{1 - l^2_{41} - l^2_{42} - l^2_{43}}
    \end{pmatrix}
\end{equation}

::: {.notes}
To simplify computation by removing redundant effects and ensure that the resulting estimate matrix is positive definite
:::

---

If we multiply $\textbf{L}$ by the random effect standard deviations, $\boldsymbol{\tau}$, and scale it with a standard normally distributed $\boldsymbol{z}$, we obtain $\textbf{v}$

\begin{equation}
    \textbf{v} = \boldsymbol{\tau}\textbf{L}\boldsymbol{z}.
\end{equation}

\begin{equation}
\begin{aligned}
    u_{0j} &= \tau_{u_0}z^\mu_j\\
    u_{1j} &= \tau_{u_1}(\rho_{u_{0j}u_{1j}} + \sqrt{1-l_{21}^2})z^\mu_j\\
    t_{0j} &= \tau_{t_0}(\rho_{u_{0j}t_{0j}} + l_{32} + \sqrt{1-l_{31}^2-l_{32}^2})z^\sigma_j\\
    t_{1j} &= \tau_{t_1}(\rho_{u_{0j}t_{1j}} + l_{42} + l_{43} + \sqrt{1-l_{41}^2-l_{31}^2-l_{32}^2})z^\sigma_j
\end{aligned}
\end{equation}

---


## Variable selection

An indicator variable ($\delta$) is included in the prior to allow switching between the spike and slab  throughout the Markov Chain Monte Carlo (MCMC) sampling process.

\begin{equation}
\begin{aligned}
    u_{0j} &= \tau_{u_0}z^\mu_j\\
    u_{1j} &= \tau_{u_1}(\rho_{u_{0j}u_{1j}} + \sqrt{1-l_{21}^2})z^\mu_j\\
    t_{0j} &= \tau_{t_0}\color{red}{\delta_{0j}}(\rho_{u_{0j}t_{0j}} + l_{32} + \sqrt{1-l_{31}^2-l_{32}^2})z^\sigma_j\\
    t_{1j} &= \tau_{t_1}\color{red}{\delta_{1j}}(\rho_{u_{0j}t_{1j}} + l_{42} + l_{43} + \sqrt{1-l_{41}^2-l_{31}^2-l_{32}^2})z^\sigma_j
\end{aligned}
\end{equation}

::: {.notes}
A Monte Carlo process refers to a simulation that samples many random values from a posterior distribution of interest.
:::

## The Spike-and-slab approach

::: columns
::: {.column width="50%"}

![Rouder et al. (2018)](rouder2018.png){fig-align="center" width=65%}

:::

::: {.column width="50%"}

- A two component mixture
a. A "spike", that is a Dirac measure concentrated at zero.
b. A diffuse "slab" component surrounding zero.
:::


:::

::: {.notes}
A Dirac measure is a measure whose (unit) mass is concentrated on a single point x of a space X.
:::

## The Spike-and-slab approach

- The subscript to the indicator ($\delta_j$) assigns each school a prior inclusion probability.

- For each school we obtain a probabilistic measure on whether the random effect is to be included or not.

- When a 0 is sampled, the portion after the fixed effect drops out of the equation.


:::fragment
\begin{equation}
\label{eq:mm_delta}
\alpha_{0j} = \begin{cases}
\eta_{00}, & \text{if }\delta_j = 0 , \\
\eta_{00} + t_{0j}, & \text{if }\delta_j = 1
\end{cases}.
\end{equation}
:::

::: {.notes}
Thus, for each iteration, this specification allows each individual to have their own person-specific estimate or the fixed effect average. 
:::
---

The posterior inclusion probabilities (PIP) can then be computed as

\begin{align}
Pr(\alpha_{0j} = \eta_{00} | \textbf{Y}) = 1 - \frac{1}{S} \sum_{s = 1}^S \delta_{js},
\end{align}

where $S = \{1,...,s\}$ denotes the posterior samples.

Assuming equal prior odds

\begin{align}
BF_{01} = \frac{Pr(\alpha_{0j} = \eta_{00} | \textbf{Y})}{1- Pr(\alpha_{0j} = \eta_{00} | \textbf{Y})}
\end{align}


## Model formulation

### Likelihood

\begin{align}
\label{eq:likelihood}
y_{ij} &\sim \mathcal{N}(\mu_{j}, \sigma^2_{j})\\
\mu_j &= X^\mu_{kij}\gamma_{k} + Z^\mu_{pj}u_{pj}\\
\sigma^2_j &= \exp(X^\sigma_{kij}\eta_k + Z^\sigma_{pj}t_{pj})\\
\textbf{v} &= \boldsymbol{\tau}\textbf{L}\color{red}{\boldsymbol{\delta}}\textbf{z}\\

\delta_{pj} &\sim \text{Bernoulli}(0.5)\\
z_{pj} &\sim \mathcal{N}(0, 1)
\end{align}



## Model formulation

### Priors

Fixed effects location:

\begin{equation}
  \gamma_{k} \sim  \mathcal{N}(0, 1000)
\end{equation}

Fixed effects scale:

\begin{equation}
  \eta_{k} \sim  \mathcal{N}(0, 1000)
\end{equation}

## Model formulation

### Priors

Random effects standard deviation:

\begin{equation}
  \tau_{p} \sim  \text{Gamma}(1, 3)
\end{equation}

Random effects correlation:

\begin{aligned}
  \textbf{L} &\sim \text{LKJ}(\eta = 1)\\
  \boldsymbol{\Omega} &= \textbf{L}'\textbf{L}
\end{aligned}


## Illustrative example

- The data for this empirical example comes from the 1982 study "High School and Beyond" (Raudenbush & Bryk, 2002)

- `mAch`: a numeric vector of Mathematics achievement scores.

- `ses`: a numeric vector of socio-economic scores.

- `sector`: a factor with levels `Public` and `Catholic`.

## Model formulation

Location

\begin{aligned}
  \text{mAch}_{ij} &= \gamma_0 + u_{0j} + (\gamma_1 + u_{1j})\text{SES}_{ij} + \gamma_2\text{Sector}_{ij} + e_{ij}
\end{aligned}

$$
e_{ij} \sim \mathcal{N} (0, \sigma_{\epsilon_{ij}}^2)
$$

Scale

\begin{aligned}
  \sigma_{\epsilon_{ij}}^2 = \exp(\eta_0 + t_{0j} + \eta_1 \text{SES}_{ij})
\end{aligned}

## Software and estimation

- The model was fitted using `NIMBLE` package in `R` (de Valpine, et al., 2017). 

- [NIMBLE](https://r-nimble.org/what-is-nimble) is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods."

- The fitted model used four chains of 1,000 iterations after a burn-in period of 1,000 iterations which resulted in a total of 2,000 samples from the posterior distribution.

- The Gelman and Rubin's (1992) scale reduction factor $\hat{R}$ was lower than 1.1 for all parameters. This ensures that all samples mixed well and sampled from the typical set.

::: {.notes}
The ^R statistic measures the ratio of the average variance of samples within each chain to the variance of the pooled samples across chains; if all chains are at equilibrium, these will be the same and ^R will be one. If the chains have not converged to a common distribution, the ^R statistic will be greater than one.
:::

## Partial results

![](img/pip_plot.png){fig-align="center"}

::: {.notes}
PIPs ranged from .25 to 1, in supporting of including random effects. This spread of PIPs indicates considerable fluctuations in the level of support for whether schools differ from the average variance in math achievement. They span from ‘moderate’ evidence in favor of belonging to the average effect on one end to ‘extreme’ evidence in favor of different from it on the other.
:::


## Partial results

![](img/funnel_plot.png){fig-align="center"}

::: {.notes}
The PIPs make a V-shape in that they decrease as the magnitude of the effect approaches zero and increase again as they move away from zero. Individuals with larger random effects should have more evidence to support that they differ from the average effect. This plot shows that mostly consistent schools should have a random effect on the variance.
:::

## Partial results

![](img/avg.png){fig-align="center"}

## Next steps

- Finish the `ivd` package for R.

- Build a ShinyApp to illustrate the package functionalities.

- Communicate the results to the scientific community.

<!-- we use spike and slab as a way of identifying those who are more consitent/inconsistent. -->

<!-- threshold of p >.75 because that reflects a BF > 3 -->

<!-- why not cutoff standard deviation? because how would we define it? also advantages of bayesian modeling? -->
