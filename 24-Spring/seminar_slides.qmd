---
title: "Beyond Average Scores: Identification of Consistent and Inconsistent Academic Achievement"
subtitle: "Quantitative Research Seminar"
author: "Marwin Carmo"
format: 
  revealjs:
    scrollable: true
    incremental: true
    code-tools: true
    code-copy: true
    code-line-numbers: true
    code-link: true
    preview-links: true
    slide-number: true
    self-contained: true
    fig-height: 4
    fig-width: 6
    fig-align: center
    margin-left: "0"
    margin-right: "0"
    width: 1400
    # height: 900
    footer: "Beyond Average Scores: Identification of Consistent and Inconsistent Academic Achievement"
    logo: "https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/01-week1-intro/02-code/02-images/ucdavis_logo_blue.png"
editor: source
editor_options: 
  chunk_output_type: console
---

## Motivation

- Average performance does not provide information on the *consistency* of academic achievement over time or within a cluster.

- Consider two students with a final grade of 75% at the year's end:
  - One had scores ranging from 50% to 100%.
  - The other maintained a steady 75%.

- Inconsistent performance may reflect unaccounted factors influencing learning.

## Our approach

- Identify clustering units (students, classrooms, etc.) that exhibit unusually high or low consistency (residual variance) in academic achievement.

- Mixed effects location scale model (MELSM) can be used to model residual variances.

- We present an adaptation of the MELSM by means of shrinking random effects to their fixed effect using the Spike and Slab regularization

## MELSM

- Educational data is classically analyzed by means of multilevel or mixed effects models (MLM):
  - Schools are level-2 units and students' test scores are the level-1 units.

- MLM assumes constant residual variance (scale)

- MELSM include a sub-model to addresses potential differences in the residual variance.

- The unexplained residual variance might provide insights over and above the expected academic achievement
that is captured in the conditional means, or location parameter.

## MELSM

- MELSM does not model the residuals themselves but, instead, the error \textit{variance} $\boldsymbol{\varphi}$ from which the residuals are realized.

- MELSM \textit{simultaneously} estimate the means and the residual variance.

- The model specifies a classic multilevel model for the observed values, $y_{ij}$ for school $j$ and student $i$, and a multilevel model for the within-cluster residual variances $\sigma^2_{\epsilon_{ij}}$.

## Advantages of MELSM

- Account for possible correlations among location and scale effects.

- For example, the variability of student performance within a school can be modeled as a function of parental socioeconomic status.

- Overcomes the limitations of multi-stage approaches and outperform the classic mixed effects model in in- and out of sample performance.

---

The starting point is the standard linear mixed effects model for $i = 1,2,...,n_j$ students, and $j = 1, 2, \ldots, S$ schools, specified as


\begin{equation}
  \label{eq:mixedmodel-lvl1}
  \textrm{Level 1:} \quad y_{ij} = \beta_{0j} + \beta_{1j} X_{ij} + e_{ij}
\end{equation}

\begin{aligned}
  \label{eq:mixedmodel-lvl2}
  \textrm{Level 2:}\quad \beta_{0j} &= \gamma_{00} + \gamma_{01}W_j + u_{0j} \\
                       \beta_{1j} &= \gamma_{10} + \gamma_{11}W_j + u_{1j}
\end{aligned}

\begin{equation}
  \label{eq:bcov}
  \begin{pmatrix} u_0 \\ u_1 \end{pmatrix} \sim \mathcal{N}
    \begin{Bmatrix} 
        \begin{pmatrix} 0 \\ 0  \end{pmatrix},
        \begin{pmatrix} \tau^2_{u_{0}} \\ \tau_{u_{0}u_{1}} & \tau^2_{u_{1}}
        \end{pmatrix}
    \end{Bmatrix}
\end{equation}

\begin{equation}
  e_{ij} \sim \mathcal{N} (0, \sigma_{\epsilon}^2)
\end{equation}

---

Next, we introduce a scale model for the residual variance

\begin{equation}
  \label{eq:var-lvl1}
  \textrm{Level 1:} \quad \sigma^2_{\epsilon_{ij}} = \exp(\alpha_{0j} + \alpha_{1j}M_{ij})
\end{equation}

\begin{aligned}
  \label{eq:var-lvl2}
  \textrm{Level 2:}\quad \alpha_{0j} &= \eta_{00} + \eta_{01}P_j + t_{0j} \\
                       \alpha_{1j} &= \eta_{10} + \eta_{11}P_j + t_{1j}
\end{aligned}

---

The random effects from the *scale* and *location* of the model come from a multivariate Gaussian Normal distribution with zero means:

\begin{equation}
  \label{eq:random-cov}
  \textbf{v}=
  \begin{bmatrix} u_0 \\ u_1 \\ t_0 \\ t_1 \end{bmatrix} \sim \mathcal{N}
    \begin{Bmatrix} 
        \boldsymbol{0}=
        \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix},
        \boldsymbol{\Sigma}=
        \begin{bmatrix} \tau^2_{u_0} \\ 
                        \tau_{u_0u_1} & \tau^2_{u_1} \\
                        \tau_{u_0t_0} & \tau_{u_1t_0} & \tau^2_{t_0} \\
                        \tau_{u_0t_1} & \tau_{u_1t_1} & \tau_{t_0t_1} & \tau^2_{t_1}
        \end{bmatrix}
    \end{Bmatrix}
\end{equation}

---

- $\boldsymbol{\Sigma}$ can be decomposed into $\boldsymbol\Sigma = \boldsymbol{\tau}\boldsymbol{\Omega\tau}'$ to specify independent priors for each element of $\boldsymbol{\tau}$ and $\boldsymbol{\Omega}$.

- $\boldsymbol{\tau}$ is a diagonal matrix where the diagonal elements are the random-effect variances.

- $\boldsymbol{\Omega}$ is the correlation matrix that contains the covariances among all random effects.

## Cholesky transformation

\begin{equation}
\label{eq:cholesky_approach}
\textbf{L} = 
    \begin{pmatrix}
        1 & 0 & 0 & 0\\
        \rho_{u_0u_1} & \sqrt{1 - l^2_{21}} & 0 & 0 \\
        \rho_{u_0t_0} & \frac{\rho_{u_1t_0} - l_{21} l_{31}}{l_{22}} & \sqrt{1 - l^2_{31} - l^2_{32}} & 0\\
        \rho_{u_0t_1} & \frac{\rho_{u_1t_1} - l_{21} l_{41}}{l_{22}} & \frac{\rho_{t_0t_1} - l_{31} l_{41} - l_{32} l_{42}}{l_{33}} & \sqrt{1 - l^2_{41} - l^2_{42} - l^2_{43}}
    \end{pmatrix}
\end{equation}

---

If we multiply $\textbf{L}$ by the random effect standard deviations, $\boldsymbol{\tau}$, and scale it with a standard normally distributed $\boldsymbol{z}$, we obtain $\textbf{v}$

\begin{equation}
    \textbf{v} = \boldsymbol{\tau}\textbf{L}\boldsymbol{z}.
\end{equation}

\begin{equation}
\begin{aligned}
    u_{0j} &= \tau_{u_0}z^\mu_j\\
    u_{1j} &= \tau_{u_1}(\rho_{u_{0j}u_{1j}} + \sqrt{1-l_{21}^2})z^\mu_j\\
    t_{0j} &= \tau_{t_0}(\rho_{u_{0j}t_{0j}} + l_{32} + \sqrt{1-l_{31}^2-l_{32}^2})z^\sigma_j\\
    t_{1j} &= \tau_{t_1}(\rho_{u_{0j}t_{1j}} + l_{42} + l_{43} + \sqrt{1-l_{41}^2-l_{31}^2-l_{32}^2})z^\sigma_j
\end{aligned}
\end{equation}

---


## Variable selection

\begin{equation}
\begin{aligned}
    u_{0j} &= \tau_{u_0}z^\mu_j\\
    u_{1j} &= \tau_{u_1}(\rho_{u_{0j}u_{1j}} + \sqrt{1-l_{21}^2})z^\mu_j\\
    t_{0j} &= \tau_{t_0}\color{red}{\delta_{0j}}(\rho_{u_{0j}t_{0j}} + l_{32} + \sqrt{1-l_{31}^2-l_{32}^2})z^\sigma_j\\
    t_{1j} &= \tau_{t_1}\color{red}{\delta_{1j}}(\rho_{u_{0j}t_{1j}} + l_{42} + l_{43} + \sqrt{1-l_{41}^2-l_{31}^2-l_{32}^2})z^\sigma_j
\end{aligned}
\end{equation}


## The Spike-and-slab approach

::: columns
::: {.column width="50%"}

- A two component mixture
a. A "spike", that is a Dirac measure concentrated at zero.
b. A diffuse "slab" component surrounding zero.
:::

::: {.column width="50%"}
:::fragment
![Rouder et al. (2018)](rouder2018.png){fig-align="center" width=65%}
:::
:::
:::

::: {.notes}
A Dirac measure is a measure whose (unit) mass is concentrated on a single point x of a space X.
:::

## The Spike-and-slab approach

- An indicator variable ($\delta$) is included in the prior to allow switching between the spike and slab  throughout the MCMC sampling process.


- The subscript to the indicator ($\delta_i$) assigns each person a prior inclusion probability.

- For example, this allows for switching between a fixed effect $t_{pj} = 0$ ($\mathcal{M}_0$) and random effects model $t_{pj} > 0$ ($\mathcal{M}_u$)--i.e.,


:::fragment
\begin{equation}
t_{pj} = \begin{cases}
0, & \text{if }\delta_{pj} = 0 , \\
t_{pj}, & \text{if }\delta_{pj} = 1
\end{cases}
\end{equation}
:::

---

The posterior model probabilities can then be computed as

\begin{align}
Pr(\mathcal{M}_u | \textbf{Y}) = \frac{1}{S} \sum_{s = 1}^S \delta_s,
\end{align}

where $S = \{1,...,s\}$ denotes the posterior samples.

Assuming equal prior odds

\begin{align}
BF = \frac{1 - Pr(\mathcal{M}_u | \textbf{Y})}{Pr(\mathcal{M}_u | \textbf{Y})}
\end{align}


## Model formulation

### Likelihood

\begin{align}
\label{eq:likelihood}
y_{ij} &\sim \mathcal{N}(\mu_{j}, \sigma^2_{j})\\
\mu_j &= X^\mu_{kij}\gamma_{k} + Z^\mu_{pj}u_{pj}\\
\sigma^2_j &= \exp(X^\sigma_{kij}\eta_k + Z^\sigma_{pj}t_{pj})\\
\textbf{v} &= \boldsymbol{\tau}\textbf{L}\color{red}{\boldsymbol{\delta}}\textbf{z}\\

\delta_{pj} &\sim \text{Bernoulli}(0.5)\\
z_{pj} &\sim \mathcal{N}(0, 1)
\end{align}



## Model formulation

### Priors

Fixed effects location:

\begin{equation}
  \gamma_{k} \sim  \mathcal{N}(0, 1000)
\end{equation}

Fixed effects scale:

\begin{equation}
  \eta_{k} \sim  \mathcal{N}(0, 1000)
\end{equation}

## Model formulation

### Priors

Random effects standard deviation:

\begin{equation}
  \tau_{p} \sim  \text{Gamma}(1, 3)
\end{equation}

Random effects correlation:

\begin{aligned}
  \textbf{L} &\sim \text{LKJ}(\eta = 1)\\
  \boldsymbol{\Omega} = \textbf{L}'\textbf{L}
\end{aligned}


## Illustrative example

- The data for this empirical example come from the Highschool and Beyond study (Raudenbush & Bryk, 2002)

- We are interested in identifying clustering units (schools) that exhibit either unusually large or unusually small within-cluster variance â€“ indicating either inconsistent or consistent academic achievement.

## Estimation

we use spike and slap as a way of identifying those who are more consitent/inconsistent.

threshold of p >.75 because that reflects a BF > 3

why not cutoff standard deviation? because how would we define it? also advantages of bayesian modeling?


