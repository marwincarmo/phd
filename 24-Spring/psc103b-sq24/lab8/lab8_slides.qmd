---
title: "Lab 8: Variance, Covariance, Correlation"
subtitle: "PSC 103B"
author: "Marwin Carmo"
format: 
  revealjs:
    scrollable: true
    incremental: true
    code-tools: true
    code-copy: true
    code-line-numbers: true
    code-link: true
    preview-links: true
    slide-number: true
    self-contained: true
    fig-height: 4
    fig-width: 6
    fig-align: center
    margin-left: "0"
    margin-right: "0"
    width: 1400
    callout-icon: false
    # height: 900
    footer: "PSC 103B - Statistical Analysis of Psychological Data"
    logo: "https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/01-week1-intro/02-code/02-images/ucdavis_logo_blue.png"
editor: source
execute:
  echo: true
editor_options: 
  chunk_output_type: console
---

## Read in data

For today's class, we're going to use the same NPAS dataset from Lab 5

```{r}
library(tidyverse)
library(lm.beta)
```

```{r}
nerdy <- read_csv("NPAS_Lab5.csv")
```

## Covariance and Correlation

- Up until now, when we have worked with more than one variable it has typically been that one of the variables is categorical, and the other is continuous.

- Covariance and correlation are summary statistics that quantify the relation between two (or more!) continuous (or ordinal) variables without making any statement about the direction.

## Covariance

- Recall that to get the variance of just a single variable, you can use the `var()` function.

:::fragment
```{r}
var(nerdy$nerdy_scale)
```
:::

- The variance quantifies the spread of the variable around its mean.

## Covariance

- The covariance looks at how the spread in one variable corresponds to the spread of the other variable.

- Or in other words, whether or not the two variables travel together or in opposite directions.

- Let's find the covariance between how extravered (`TIPI6`) and someone is and how reserved (`TIPI1`) they are

:::fragment
```{r}
cov(nerdy$TIPI1, nerdy$TIPI6)
```
:::

- What does this tell us about the relation between these two variables?

## Covariance

- We can also get all pairwise covariances of multiple variables by simply supplying the `cov()` function:

:::fragment
```{r}
#| error: true
cov(nerdy)
```
:::

:::fragment
```{r}
nerdy_items <- nerdy |> 
  select(TIPI1:TIPI3)

cov(nerdy_items)
```
:::

:::{.notes}
Wait, we got an error! Why is that?
It's because some of our variables (mainly, country
and continent) are not numeric, so we cannot calculate
a covariance with them
:::

## Covariance

```{r echo=FALSE}
cov(nerdy_items)
```

- The output of this is called a [variance-covariance matrix](https://www.cuemath.com/algebra/covariance-matrix/) and it describes (as the name suggests) the variances and covariances of the variables in your dataset.

- The diagonal elements of the variance-covariance matrix are the variances of the variables. Why? Because the way that a variable relates to itself is just its spread.

- The off-diagonal elements are the covariances between each pair of variables. Notice that this matrix is symmetric.

## Covariance

- How can we interpret these values? For example, the covariance between temperature and sales is `r round(cov(nerdy$TIPI1, nerdy$TIPI6),2)` -- what does this tell us about their relation?

- We can only tell that the relation between the two is **negative** -- that is, the higher the extraversion, the less reserved someone is. But is this relation really strong? That's not something we can get from the covariance, and is why we turn to correlation instead!

## Correlation

- Correlation is a standardized covariance, and since it can only fall between -1 and 1.

- It makes it easier to interpret the value as a measure of the strength of the relation.

:::fragment
```{r}
cor(nerdy_items)
```
:::

- Which relations appear to be strong? Which ones appear to be weak?

## Significance Testing for a Correlation

- We calculated our correlations, but we aren't able to tell whether these values are represent significant
relations -- significant meaning that the correlation coefficient is different from 0. 

- First, let's plot our data -- we want to make sure that using the correlation to quantify the relationship between the variables is suitable, because correlation and covariance can only capture linear relations. 

- We also don't want to necessarily use correlation if there are strong outliers in our data, as those outliers can change the value of our correlation coefficient.

---

```{r }
#| fig-align: "center"
plot(nerdy$TIPI6, nerdy$nerdy_scale, xlab = "Reservedness", ylab = "Nerdiness")
```

## Significance Testing for a Correlation

- Our null and alternative hypotheses are:

  - $H_0$: $\rho$ = 0
  - $H_1$: $\rho \neq$ 0

- This is a **two-sided test** -- either there is a relation or there isn't a relation.

- We're not specifying which direction we expect that relation to be in.

## Significance Testing for a Correlation

```{r}
cor.test(nerdy$TIPI6, nerdy$nerdy_scale, method = "pearson")
```

