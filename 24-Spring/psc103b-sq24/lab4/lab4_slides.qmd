---
title: "Lab 4: Chi Square Tests"
subtitle: "PSC 103B"
author: "Marwin Carmo"
format: 
  revealjs:
    scrollable: true
    incremental: true
    code-tools: true
    code-copy: true
    code-line-numbers: true
    code-link: true
    preview-links: true
    slide-number: true
    self-contained: true
    fig-height: 4
    fig-width: 6
    fig-align: center
    margin-left: "0"
    margin-right: "0"
    width: 1400
    callout-icon: false
    # height: 900
    footer: "PSC 103B - Statistical Analysis of Psychological Data"
    logo: "https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/01-week1-intro/02-code/02-images/ucdavis_logo_blue.png"
editor: source
execute:
  echo: true
editor_options: 
  chunk_output_type: console
---

## Packages and data

First of all, lets load tidyverse

```{r}
library(tidyverse)
```

And read in the data with this "new" function

```{r}
cards <- read_rds('Lab4Data.rds')
```


::: {.notes}
Today the data we'll use is in an .rds file. 
This is a type of R file that allows you to save a little bit more information with your dataframe or other object (for example, if you have a column full of numbers that you
want to be stored as type character instead, saving an .rds will
allow you to do that). The downside to .rds files is that
it's not possible to view it as easily outside of R,
like a .csv file is (you could simply open it using any
spreadsheet software).
:::

## Today's dataset

- 200 people were asked to imagine a shuffled deck of cards and mentally pick one card at random. 

- Then they were asked to randomly pick another card. 

- This data set records the suit choice for the first and second choices for each participant. 

- Participants didn't  physically pick a card, they were asked to imagine randomly picking a card.

## Today's dataset

```{r}
head(cards)
```

- `id`: participant ID
- `choice_1`: card suit for first pick 
- `choice_2`: card suit for second pick 

---

```{r}
table(cards$choice_1)
```

- Any inital impressions? 

- What would it look like if people chose cards randomly?

- Do people really choose cards randomly?

## Chi Square Goodness of Fit Test

- Let's say our theory is "people don't choose cards randomly" 

- $H_0$: All four suits are chosen with equal probability, or

- $H_0$: P = (P1, P2, P3, P4), where P is a vector or set of probabilities.

- If the suits are equally represented, what proportions do we expect?

- $H_0$: P = (.25, .25, .25, .25)

:::fragment
```{r}
H0_prob <- c(clubs = .25, diamonds = .25, hearts = .25, spades = .25)
```
:::

## Chi Square Goodness of Fit Test

- $H_1$: *At least one* of the suit-choice probabilities isn't .25. How would you write that? Remember that probabilities must sum to 1.

- $H_1$: P $\neq$ (.25, .25, .25, .25)

## Chi Square Goodness of Fit Test

- We can save our observed frequencies for the first choice in an object:

:::fragment
```{r}
observed_freq <- table(cards$choice_1)

observed_freq
```
:::

- We can compare these two pieces of information to test our H0.

## Chi Square Goodness of Fit Test

- We can transform the H0 probabilities into the frequencies we would expect to see if H0 were true

- First, we need to write our total sample size:

:::fragment
```{r}
N <- 200
```
:::

- And we can multiply this by our expected probabilities to get the expected frequencies:

:::fragment
```{r}
expected_freq <- H0_prob * N
expected_freq
```
:::

## Chi Square Goodness of Fit Test

$H_0$: P = (.25, .25, .25, .25)

$H_1$: P $\neq$ (.25, .25, .25, .25)

Look at the data again. Do you think H0 or H1 is more likely to be true?

```{r}
observed_freq
expected_freq
```

## Chi Square Goodness of Fit Test

- What we want to do now is compare the expected results with the observed results.

- If they are similar enough, we don't reject H0.

- If they are very different, we reject H0.

- But how do we know how far is **too far** from the expected values? 

## Step 1: Compute "error" scores

- To conduct the GoF test, we need to compute the error score to measure the difference between what we expected and what we observed.

- We compare this to a distribution to see how big that difference is, and if it's big enough to reject the null.

- The formula for this is $(O - E)^2 / E$

:::fragment
```{r}
(error_scores <- (observed_freq - expected_freq)^2 / expected_freq)
```
:::

:::fragment
```{r}
(goodness_of_fit <- sum(error_scores)) # This is our goodness of fit statistic
```
:::

## Step 1: Compute "error" scores

The closer the observed frequencies are to the prediction by H0 (expected frequencies), the __________ the goodness of fit statistic will get. [Larger or smaller?]

- Smaller!

- How large is large enough to reject H0?

## Step 2: Compare our statistic to the chi-square distribution

Notice how the shape of the distribution changes as the degrees of freedom (df) increase.

```{r echo=FALSE}
#| fig-align: "center"

x1 <- seq(0, 30, by = .1)
y_df5 <- dchisq(x = x1, df = 5)
y_df10 <- dchisq(x = x1, df = 10)
y_df15 <- dchisq(x = x1, df = 15)

tibble(x1, y_df5, y_df10, y_df15) %>%
  pivot_longer(-x1, names_to = "dfs", names_prefix = "y_df", values_to = "value") %>%
  mutate(dfs = ordered(dfs, levels = c(5, 10, 15))) %>% 
  ggplot(aes(x = x1, y = value, color = dfs)) +
  geom_line(size = 1.5, alpha = .9) +
  labs(x = "", y = "density") +
  theme_classic()
```

## Step 2: Compare our statistic to the chi-square distribution

- To compare our statistic to the chi-square distribution, we can use the `pchisq()` function. 

- The chi-square distribution needs a df, where `df = number of categories - 1`.

::: fragment
```{r}
pchisq(q = goodness_of_fit, df = 3, lower.tail = FALSE)
```
:::

::: {.notes}
We want the probability of getting this value or *more* extreme, so we want information about the upper tail, so lower.tail = FALSE 
:::

## Step 3: Decide if our p-value is big or small

- Is the p value you got from the `pchisq()` function larger or smaller than .05? 

- Can we reject H0? 

- What does this mean?

## Step 3B: Compute the Bayes factor

```{r}
library(BayesFactor)
```


```{r}
proportionBF(y = c(35,51,64,50), N = rep(200, 4), p = .25)

```

## An easier way to test it

- We can do this very easily in R using the `chisq.test()` function. The arguments are: `chisq.test(x = observed frequencies, p = expected probabilities)`.

::: fragment
:::{.callout-caution title="Exercise"}
Try to run the  `chisq.test()` function yourself first.
:::
:::

::: fragment
```{r}
chisq.test(observed_freq, p = c(.25, .25, .25, .25))
```
:::

---

- Notice that we assumed equal probabilities for  this test, but we didn't have to; we could have expected whatever probabilities we wanted and then we would just need to specify them in `p`. 

- We also need to make sure that the order of observed frequencies and p are the same.

- For example:

::: fragment
```{r eval=FALSE}
chisq.test(observed_freq, p = c(.40, .35, .15, .10))
```
:::

## The Chi Square test of Independence

- Another type of $\chi^2$ test is the one when we have two categorical variables, and we're interested in testing whether or not they're related or dependent on each other.

- **Dependence** means that knowing the value of one variable gives you an idea of what the value on the second variable is going to be.

- **Independence** means knowing the value of one variable doesn't tell you anything about the value of the other variable.

## Chi-Square Test of Independence

- $H_0$: The choice of suit in the second imagination exercise is independent of the first.

- $H_A$: The choice of the first and the second suits are not independent.

- Just like in the GoF test, the chi-square test of independence computes the difference between what we would expect if the variables were independent, and what we observed.

## Chi-Square Test of Independence

- If `Choice1` and `Choice2` are independent variables, knowing what someone picked as their first choice won't help us guess what they picked as their second choice. 

- Another way to think about this is that the probability of someone picking a suit for `Choice1` is not related to the probability of picking a suit for `Choice2`.

## Chi-Square Test of Independence

We can use the `table()` function to get the number of observations for each combination of two variables, like this:

```{r}
cont_tab<-table(Choice1 = cards$choice_1, Choice2 = cards$choice_2)
cont_tab
```

## Chi-Square Test of Independence

- Since our goal is to see if two variables are independent, we need to give the function both variables.

- When we do that, by default the function will do a chi-square test of independence between the two categorical variables.

:::fragment
```{r}
chisq.test(x=cards$choice_1, y = cards$choice_2)
```
:::

- We could also use the matrix we created earlier `chisq.test(cont_tab)`

## Chi-Square Test of Independence with Bayes factor

```{r}
contingencyTableBF(x = cont_tab, sampleType = "indepMulti", fixedMargin = "cols")
```

What can we conclude about the two variables? 

## Now you try

- You will use a data set built-in in `R` named `UCBAdmissions` . 

- It refers to aggregate data on applicants to graduate school at Berkeley for the six largest departments in 1973 classified by admission and sex.

- It is a 3-dimensional array resulting from cross-tabulating 4526 observations on 3 variables. Look at it by calling `UCBAdmissions` on your console.

## Now you try

Aggregate data over departments running the following code:

```{r}
agg_UCBAdmissions <- apply(UCBAdmissions, c(1, 2), sum)
agg_UCBAdmissions
```

What would be your null and alternative hypothesis? What you expect to find. 

With this new data set, run a chi-squared test to check if the data show evidence of sex bias in admission practices. Compute either a p-value or a BF (or both!).




